# RAG Chatbot Project Guide

## What is RAG?

Retrieval-Augmented Generation (RAG) is a technique that enhances large language models by incorporating external knowledge sources. Instead of relying solely on the model's training data, RAG systems retrieve relevant information from a knowledge base and use it to ground the model's responses.

## How RAG Works

The RAG pipeline consists of three main stages:

### 1. Document Ingestion
Documents are loaded, parsed, and split into chunks. Each chunk is a manageable piece of text that can be individually embedded. This process involves reading the raw documents and breaking them into smaller units while preserving context and metadata.

### 2. Vector Embedding and Indexing
Each chunk is converted into a dense vector representation using an embedding model. In this system, we use OpenAI's embedding models to convert text into high-dimensional vectors. These vectors are then indexed using FAISS (Facebook AI Similarity Search), which enables fast similarity-based retrieval.

### 3. Retrieval and Generation
When a user asks a question, it is converted into an embedding and compared against the indexed chunks. The most similar chunks are retrieved and used as context for the language model. The model generates a response that is grounded in the retrieved context, and citations are provided pointing back to the source chunks.

## System Architecture

The RAG system consists of four main components:

- **DocumentIngestor**: Loads and chunks documents
- **RAGSystem**: Manages embeddings, FAISS indexing, and retrieval
- **MemoryManager**: Extracts and stores important insights
- **RAGChatbot**: Provides the user interface

## Key Features

### Citations
Every answer includes citations that show which parts of the ingested documents were used. Citations include the source document, chunk ID, relevant snippet, and a relevance score.

### Memory Management
The system automatically extracts high-signal insights from conversations and stores them in markdown files:
- USER_MEMORY.md: User-specific facts and preferences
- COMPANY_MEMORY.md: Organization-wide learnings and patterns

### Fault Tolerance
If retrieval returns no relevant results, the system gracefully indicates that the question cannot be answered based on available documents.

## Typical Workflow

1. User starts the chatbot
2. User adds documents using `add <filename>` command
3. System ingests, chunks, and indexes documents
4. User asks questions using `ask <question>` command
5. System retrieves relevant chunks and generates grounded responses
6. Important insights are automatically stored in memory files

## Performance Considerations

- Chunk size affects both indexing speed and generation quality
- Embedding quality depends on the chosen embedding model
- FAISS retrieval is very fast even with millions of documents
- The system uses L2 distance for similarity search
